{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3-panww.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOSG1PHqb+jWw+5q/xZBY/5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuperPanww/courses_ML20/blob/master/hw3_panww.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhzdomRTOKoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gdown --id '19CzXudqN58R3D-1G8KeFWk8UDQwlb8is' --output food-11.zip # 下載資料集\n",
        "!unzip food-11.zip # 解壓縮"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq1XMfBs3c7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import需要的套件\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i2yiKJ87hME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readfile(path, label):\n",
        "  image_dir = sorted(os.listdir(path))\n",
        "  x = np.zeros((len(image_dir),128,128,3),dtype=np.uint8)\n",
        "  y = np.zeros((len(image_dir)),dtype=np.uint8)\n",
        "  for i,file in enumerate(image_dir):\n",
        "    img = cv2.imread(os.path.join(path, file))\n",
        "    x[i,:] = cv2.resize(img,(128,128))\n",
        "    if label:\n",
        "      y[i] = int(file.split(\"_\")[0])\n",
        "  if label:\n",
        "    return x,y\n",
        "  else:\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWtJKKxn9So0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "bba6e2a0-0a5e-44f6-9044-e06e15f8f938"
      },
      "source": [
        "workspace_dir = \"./food-11\"\n",
        "print('Reading data')\n",
        "train_x,train_y = readfile(os.path.join(workspace_dir,'training'),True)\n",
        "print(\"Size of training data = {}\".format(len(train_x)))\n",
        "val_x, val_y = readfile(os.path.join(workspace_dir,\"validation\"),True)\n",
        "print(\"Size of validation data = {}\".format(len(val_x)))\n",
        "test_x = readfile(os.path.join(workspace_dir,\"testing\"),False)\n",
        "print(\"Size of Testing data = {}\".format(len(test_x)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading data\n",
            "Size of training data = 9866\n",
            "Size of validation data = 3430\n",
            "Size of Testing data = 3347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbFuU0JfAaWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),                                     \n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "class ImgDataset(Dataset):\n",
        "  def __init__(self,x,y = None, transform = None):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    if y is not None:\n",
        "      self.y = torch.LongTensor(y)\n",
        "    self.transform = transform\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "  def __getitem__(self, index):\n",
        "    X = self.x[index]\n",
        "    if self.transform is not None:\n",
        "      X = self.transform(X)\n",
        "    if self.y is not None:\n",
        "      Y = self.y[index]\n",
        "      return X,Y\n",
        "    else:\n",
        "      return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLU8fg7dGUVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "train_set = ImgDataset(train_x,train_y, train_transform)\n",
        "val_set = ImgDataset(val_x,val_y,test_transform)\n",
        "train_loader = DataLoader(train_set,batch_size = batch_size,shuffle=True)\n",
        "val_loader = DataLoader(val_set,batch_size=batch_size,shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvyJY2GZHjOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Classifier, self).__init__()\n",
        "\n",
        "    self.cnn = nn.Sequential(\n",
        "        nn.Conv2d(3,64,3,1,1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2,0),\n",
        "\n",
        "        nn.Conv2d(64,128,3,1,1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2,0),\n",
        "\n",
        "        nn.Conv2d(128,256,3,1,1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2,0),\n",
        "\n",
        "        nn.Conv2d(256,512,3,1,1),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2,0),\n",
        "\n",
        "        nn.Conv2d(512,512,3,1,1),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2,0),\n",
        "    )\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(512*4*4, 1024),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(1024, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 11)       \n",
        "    )\n",
        "  def forward(self, x):\n",
        "    out = self.cnn(x)\n",
        "    out = out.view(out.size()[0], -1)\n",
        "    return self.fc(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PkmNjRTKhFN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "outputId": "258806f2-0de9-496a-86f6-3fb2fc916ec5"
      },
      "source": [
        "model = Classifier().cuda()\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
        "num_epoch = 30\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "  epoch_start_time = time.time()\n",
        "  train_acc = 0.0\n",
        "  train_loss = 0.0\n",
        "  val_acc = 0.0\n",
        "  val_loss = 0.0\n",
        "\n",
        "  model.train()\n",
        "  for i, data in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    train_pred = model(data[0].cuda())\n",
        "    batch_loss = loss(train_pred, data[1].cuda())\n",
        "    batch_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
        "    train_loss += batch_loss.item()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(val_loader):\n",
        "      val_pred = model(data[0].cuda())\n",
        "      batch_loss = loss(val_pred, data[1].cuda())\n",
        "\n",
        "      val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis = 1) == data[1].numpy())\n",
        "      val_loss += batch_loss.item()\n",
        "    \n",
        "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
        "          (epoch + 1,num_epoch, time.time() - epoch_start_time, \\\n",
        "           train_acc/train_set.__len__(),train_loss/train_set.__len__(),val_acc/val_set.__len__(), val_loss/val_set.__len__()))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[001/030] 24.73 sec(s) Train Acc: 0.221265 Loss: 0.018186 | Val Acc: 0.237026 loss: 0.016879\n",
            "[002/030] 24.73 sec(s) Train Acc: 0.324245 Loss: 0.015040 | Val Acc: 0.242566 loss: 0.021010\n",
            "[003/030] 24.68 sec(s) Train Acc: 0.396919 Loss: 0.013714 | Val Acc: 0.136443 loss: 0.024250\n",
            "[004/030] 24.69 sec(s) Train Acc: 0.439286 Loss: 0.012794 | Val Acc: 0.375219 loss: 0.015114\n",
            "[005/030] 24.72 sec(s) Train Acc: 0.469086 Loss: 0.012199 | Val Acc: 0.280466 loss: 0.018427\n",
            "[006/030] 24.72 sec(s) Train Acc: 0.505676 Loss: 0.011276 | Val Acc: 0.349854 loss: 0.017036\n",
            "[007/030] 24.73 sec(s) Train Acc: 0.534259 Loss: 0.010724 | Val Acc: 0.202624 loss: 0.025183\n",
            "[008/030] 24.74 sec(s) Train Acc: 0.548652 Loss: 0.010220 | Val Acc: 0.438776 loss: 0.014461\n",
            "[009/030] 24.75 sec(s) Train Acc: 0.573586 Loss: 0.009655 | Val Acc: 0.412536 loss: 0.015859\n",
            "[010/030] 24.76 sec(s) Train Acc: 0.602574 Loss: 0.009073 | Val Acc: 0.556851 loss: 0.010577\n",
            "[011/030] 24.76 sec(s) Train Acc: 0.616359 Loss: 0.008731 | Val Acc: 0.477259 loss: 0.012613\n",
            "[012/030] 24.83 sec(s) Train Acc: 0.636935 Loss: 0.008144 | Val Acc: 0.576093 loss: 0.010183\n",
            "[013/030] 24.79 sec(s) Train Acc: 0.662781 Loss: 0.007697 | Val Acc: 0.496793 loss: 0.012695\n",
            "[014/030] 24.79 sec(s) Train Acc: 0.669065 Loss: 0.007374 | Val Acc: 0.427697 loss: 0.016209\n",
            "[015/030] 24.77 sec(s) Train Acc: 0.686600 Loss: 0.007077 | Val Acc: 0.335277 loss: 0.022778\n",
            "[016/030] 24.74 sec(s) Train Acc: 0.697446 Loss: 0.007023 | Val Acc: 0.508746 loss: 0.012510\n",
            "[017/030] 24.80 sec(s) Train Acc: 0.717819 Loss: 0.006411 | Val Acc: 0.587172 loss: 0.010300\n",
            "[018/030] 24.75 sec(s) Train Acc: 0.729880 Loss: 0.006153 | Val Acc: 0.540233 loss: 0.012266\n",
            "[019/030] 24.70 sec(s) Train Acc: 0.744375 Loss: 0.005873 | Val Acc: 0.648688 loss: 0.008609\n",
            "[020/030] 24.70 sec(s) Train Acc: 0.753801 Loss: 0.005511 | Val Acc: 0.576093 loss: 0.011307\n",
            "[021/030] 24.65 sec(s) Train Acc: 0.767788 Loss: 0.005143 | Val Acc: 0.537609 loss: 0.013384\n",
            "[022/030] 24.67 sec(s) Train Acc: 0.769005 Loss: 0.005105 | Val Acc: 0.531195 loss: 0.013964\n",
            "[023/030] 24.68 sec(s) Train Acc: 0.776201 Loss: 0.005041 | Val Acc: 0.563265 loss: 0.011988\n",
            "[024/030] 24.65 sec(s) Train Acc: 0.783296 Loss: 0.004893 | Val Acc: 0.570554 loss: 0.012156\n",
            "[025/030] 24.70 sec(s) Train Acc: 0.781776 Loss: 0.004938 | Val Acc: 0.628571 loss: 0.009928\n",
            "[026/030] 24.70 sec(s) Train Acc: 0.820596 Loss: 0.004077 | Val Acc: 0.361808 loss: 0.022946\n",
            "[027/030] 24.63 sec(s) Train Acc: 0.808028 Loss: 0.004251 | Val Acc: 0.531487 loss: 0.014376\n",
            "[028/030] 24.69 sec(s) Train Acc: 0.833367 Loss: 0.003702 | Val Acc: 0.665889 loss: 0.009500\n",
            "[029/030] 24.67 sec(s) Train Acc: 0.854044 Loss: 0.003302 | Val Acc: 0.583382 loss: 0.013454\n",
            "[030/030] 24.62 sec(s) Train Acc: 0.843402 Loss: 0.003465 | Val Acc: 0.667347 loss: 0.009748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItsI2qoaywAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_val_x = np.concatenate((train_x,val_x), axis = 0)\n",
        "train_val_y = np.concatenate((train_y,val_y), axis = 0)\n",
        "train_val_set = ImgDataset(train_val_x, train_val_y, train_transform)\n",
        "train_val_loader = DataLoader(train_val_set, batch_size = batch_size, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3Pn7NHG484l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "outputId": "442cb418-644a-45ee-9f31-2bb0e41207b5"
      },
      "source": [
        "model_best = Classifier().cuda()\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_best.parameters(), lr= 0.001)\n",
        "num_epoch = 30\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "  epoch_start_time = time.time()\n",
        "  train_acc = 0.0\n",
        "  train_loss = 0.0\n",
        "  \n",
        "  model_best.train()\n",
        "  for i,data in enumerate(train_val_loader):\n",
        "    optimizer.zero_grad()\n",
        "    train_pred = model_best(data[0].cuda())\n",
        "    batch_loss = loss(train_pred, data[1].cuda())\n",
        "    batch_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis = 1) == data[1].numpy())\n",
        "    train_loss += batch_loss.item()\n",
        "\n",
        "  print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \\\n",
        "        (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
        "         train_acc/train_val_set.__len__(),train_loss/train_val_set.__len__()))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[001/030] 29.46 sec(s) Train Acc: 0.247894 Loss: 0.017272\n",
            "[002/030] 29.30 sec(s) Train Acc: 0.375526 Loss: 0.013933\n",
            "[003/030] 29.21 sec(s) Train Acc: 0.440283 Loss: 0.012606\n",
            "[004/030] 29.45 sec(s) Train Acc: 0.490298 Loss: 0.011576\n",
            "[005/030] 29.41 sec(s) Train Acc: 0.524669 Loss: 0.010653\n",
            "[006/030] 29.37 sec(s) Train Acc: 0.556258 Loss: 0.009957\n",
            "[007/030] 29.37 sec(s) Train Acc: 0.596570 Loss: 0.009057\n",
            "[008/030] 29.40 sec(s) Train Acc: 0.622819 Loss: 0.008550\n",
            "[009/030] 29.40 sec(s) Train Acc: 0.647864 Loss: 0.007919\n",
            "[010/030] 29.41 sec(s) Train Acc: 0.668096 Loss: 0.007455\n",
            "[011/030] 29.39 sec(s) Train Acc: 0.685018 Loss: 0.007084\n",
            "[012/030] 29.47 sec(s) Train Acc: 0.702843 Loss: 0.006673\n",
            "[013/030] 29.39 sec(s) Train Acc: 0.718487 Loss: 0.006295\n",
            "[014/030] 29.44 sec(s) Train Acc: 0.736613 Loss: 0.005961\n",
            "[015/030] 29.44 sec(s) Train Acc: 0.751730 Loss: 0.005479\n",
            "[016/030] 29.42 sec(s) Train Acc: 0.767900 Loss: 0.005222\n",
            "[017/030] 29.45 sec(s) Train Acc: 0.783017 Loss: 0.004874\n",
            "[018/030] 29.43 sec(s) Train Acc: 0.789786 Loss: 0.004679\n",
            "[019/030] 29.43 sec(s) Train Acc: 0.803174 Loss: 0.004402\n",
            "[020/030] 29.42 sec(s) Train Acc: 0.820171 Loss: 0.003969\n",
            "[021/030] 29.43 sec(s) Train Acc: 0.826715 Loss: 0.003857\n",
            "[022/030] 29.43 sec(s) Train Acc: 0.843637 Loss: 0.003507\n",
            "[023/030] 29.46 sec(s) Train Acc: 0.851158 Loss: 0.003315\n",
            "[024/030] 29.47 sec(s) Train Acc: 0.866727 Loss: 0.003002\n",
            "[025/030] 29.45 sec(s) Train Acc: 0.875677 Loss: 0.002727\n",
            "[026/030] 29.47 sec(s) Train Acc: 0.879212 Loss: 0.002665\n",
            "[027/030] 29.47 sec(s) Train Acc: 0.890794 Loss: 0.002387\n",
            "[028/030] 29.44 sec(s) Train Acc: 0.900421 Loss: 0.002193\n",
            "[029/030] 29.51 sec(s) Train Acc: 0.900045 Loss: 0.002154\n",
            "[030/030] 29.48 sec(s) Train Acc: 0.916968 Loss: 0.001786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIVsxiKz8IpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set = ImgDataset(test_x, transform = test_transform)\n",
        "test_loader = DataLoader(test_set, batch_size = batch_size, shuffle= False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll212WlI8vVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_best.eval()\n",
        "prediction = []\n",
        "with torch.no_grad():\n",
        "  for i, data in enumerate(test_loader):\n",
        "    test_pred = model_best(data.cuda())\n",
        "    test_label = np.argmax(test_pred.cpu().data.numpy(), axis = 1)\n",
        "    for y in test_label:\n",
        "      prediction.append(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye5W7yH99YGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"predict.cas\", 'w') as f:\n",
        "  f.write('Id,Category\\n')\n",
        "  for i ,y in enumerate(prediction):\n",
        "    f.write('{},{}\\n'.format(i,y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jQ8keX29wXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}